{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dahir/deedax/TLDR'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "  root_dir: Path\n",
    "  STATUS_FILE: str\n",
    "  ALL_REQUIRED_FILES: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TLDR.constants import *\n",
    "from TLDR.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "  def __init__(\n",
    "    self,\n",
    "    config_filepath = CONFIG_FILE_PATH,\n",
    "    params_filepath = PARAMS_FILE_PATH,\n",
    "  ):\n",
    "    self.config = read_yaml(config_filepath)\n",
    "    self.params = read_yaml(params_filepath)\n",
    "\n",
    "    create_directories([self.config.artifacts_root])\n",
    "\n",
    "  def get_data_validation_config(self) -> DataValidationConfig:\n",
    "    config = self.config.data_validation\n",
    "    \n",
    "    create_directories([config.root_dir])\n",
    "    \n",
    "    data_validation_config = DataValidationConfig(\n",
    "      root_dir = Path(config.root_dir),\n",
    "      STATUS_FILE = config.STATUS_FILE,\n",
    "      ALL_REQUIRED_FILES = config.ALL_REQUIRED_FILES,\n",
    "    )\n",
    "    \n",
    "    return data_validation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from TLDR.logging import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidation:\n",
    "  def __init__(self, config: DataValidationConfig):\n",
    "    self.config = config\n",
    "\n",
    "  def validate_all_files_exist(self) -> bool:\n",
    "    try:\n",
    "      validation_status = None\n",
    "      all_existing_files = os.listdir(os.path.join('artifacts', 'data_ingestion', 'bank-additional'))\n",
    "      for file in self.config.ALL_REQUIRED_FILES:\n",
    "        if file in all_existing_files:\n",
    "          validation_status = True\n",
    "          logger.info(f'Validation Status for {file}: {validation_status}')\n",
    "          with open(self.config.STATUS_FILE, 'w') as f:\n",
    "            f.write(f'Validation Status: {validation_status}')\n",
    "        else:\n",
    "          validation_status = False\n",
    "          logger.info(f'Validation Status for {file}: {validation_status}')\n",
    "          with open(self.config.STATUS_FILE, 'w') as f:\n",
    "            f.write(f'Validation Status: {validation_status}')\n",
    "    except Exception as e:\n",
    "      raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-17 21:45:17,832: INFO: common] yaml file: <_io.TextIOWrapper name='config/config.yaml' mode='r' encoding='UTF-8'> read successfully.]\n",
      "[2023-10-17 21:45:17,837: INFO: common] yaml file: <_io.TextIOWrapper name='params.yaml' mode='r' encoding='UTF-8'> read successfully.]\n",
      "[2023-10-17 21:45:17,839: INFO: common] Created directory: artifacts]\n",
      "[2023-10-17 21:45:17,840: INFO: common] Created directory: artifacts/data_validation]\n",
      "[2023-10-17 21:45:17,842: INFO: 177820508] Validation Status for bank-additional.csv: True]\n",
      "[2023-10-17 21:45:17,844: INFO: 177820508] Validation Status for bank-additional-full.csv: True]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  config = ConfigurationManager()\n",
    "  data_validation_config = config.get_data_validation_config()\n",
    "  data_validation = DataValidation(data_validation_config)\n",
    "  data_validation.validate_all_files_exist()\n",
    "except Exception as e:\n",
    "  raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "  root_dir: Path\n",
    "  data_path: Path\n",
    "  tokenizer_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TLDR.constants import *\n",
    "from TLDR.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigutationManager:\n",
    "  def __init__(\n",
    "    self,\n",
    "    config_filepath = CONFIG_FILE_PATH,\n",
    "    params_filepath = PARAMS_FILE_PATH,\n",
    "  ):\n",
    "    self.config = read_yaml(config_filepath)\n",
    "    self.params = read_yaml(params_filepath)\n",
    "\n",
    "    create_directories([self.config.artifacts_root])\n",
    "\n",
    "  def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "\n",
    "    config = self.config.data_transformation\n",
    "\n",
    "    create_directories([config.root_dir])\n",
    "\n",
    "    data_transformation_config = DataTransformationConfig(\n",
    "      root_dir = config.root_dir,\n",
    "      data_path = config.data_path,\n",
    "      tokenizer_name = config.tokenizer_name,\n",
    "    )\n",
    "    return data_transformation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from TLDR.logging import logger\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "  def __init__(self, config: DataTransformationConfig):\n",
    "    self.config = config\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "  def __init__(self, config: DataTransformationConfig):\n",
    "    self.config = config\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "  def convert_examples_to_features(self, example_batch):\n",
    "    input_encodings = self.tokenizer(example_batch['dialogue'], max_length = 1024, truncation = True)\n",
    "\n",
    "    with self.tokenizer.as_target_tokenizer():\n",
    "      target_encodings = self.tokenizer(example_batch['summary'], max_length = 128, truncation = True)\n",
    "\n",
    "    return {\n",
    "      'input_ids': input_encodings['input_ids'],\n",
    "      'attention_mask': input_encodings['attention_mask'],\n",
    "      'labels': target_encodings['input_ids'],\n",
    "    }\n",
    "  \n",
    "  def convert(self):\n",
    "    dset = load_from_disk(self.config.data_path)\n",
    "    dset_pt = dset.map(self.convert_examples_to_features, batched = True)\n",
    "    dset_pt.save_to_disk(os.path.join(self.config.root_dir, 'dset')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-18 01:24:17,729: INFO: common] yaml file: <_io.TextIOWrapper name='config/config.yaml' mode='r' encoding='UTF-8'> read successfully.]\n",
      "[2023-10-18 01:24:17,734: INFO: common] yaml file: <_io.TextIOWrapper name='params.yaml' mode='r' encoding='UTF-8'> read successfully.]\n",
      "[2023-10-18 01:24:17,738: INFO: common] Created directory: artifacts]\n",
      "[2023-10-18 01:24:17,740: INFO: common] Created directory: artifacts/data_transformation]\n",
      "[2023-10-18 01:24:18,221: WARNING: fingerprint] Parameter 'function'=<function DataTransformation.convert_examples_to_features at 0x7f2b8c1cb370> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?ba/s]/home/dahir/deedax/TLDR/TLDR/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3606: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.53ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.36ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.07ba/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  config = ConfigutationManager()\n",
    "  data_transformation_config = config.get_data_transformation_config()\n",
    "  data_transformation = DataTransformation(data_transformation_config)\n",
    "  data_transformation.convert()\n",
    "except Exception as e:\n",
    "  raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Model Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dahir/deedax/TLDR'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "  root_dir: Path\n",
    "  data_path: Path\n",
    "  model_ckpt: Path\n",
    "  num_train_epochs: int\n",
    "  warmup_steps: int\n",
    "  per_device_train_batch_size: int\n",
    "  weight_decay: float\n",
    "  logging_steps: int\n",
    "  evaluation_strategy: str\n",
    "  eval_steps: int\n",
    "  save_steps: float\n",
    "  gradient_accumulation_steps: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TLDR.constants import *\n",
    "from TLDR.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "  def __init__(\n",
    "    self,\n",
    "    config_filepath = CONFIG_FILE_PATH,\n",
    "    params_filepath = PARAMS_FILE_PATH,\n",
    "  ):\n",
    "    self.config = read_yaml(config_filepath)\n",
    "    self.params = read_yaml(params_filepath)\n",
    "\n",
    "    create_directories([self.config.artifacts_root])\n",
    "\n",
    "  def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "    config = self.config.model_trainer\n",
    "    params = self.params.TrainingArguments\n",
    "\n",
    "    create_directories([config.root_dir])\n",
    "\n",
    "    model_trainer_config = ModelTrainerConfig(\n",
    "      root_dir = config.root_dir,\n",
    "      data_path = config.data_path,\n",
    "      model_ckpt = config.model_ckpt,\n",
    "      num_train_epochs = params.num_train_epochs,\n",
    "      warmup_steps = params.warmup_steps,\n",
    "      per_device_train_batch_size = params.per_device_train_batch_size,\n",
    "      weight_decay = params.weight_decay,\n",
    "      logging_steps = params.logging_steps,\n",
    "      evaluation_strategy = params.evaluation_strategy,\n",
    "      eval_steps = params.eval_steps,\n",
    "      save_steps = params.save_steps,\n",
    "      gradient_accumulation_steps = params.gradient_accumulation_steps,\n",
    "    )\n",
    "\n",
    "    return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dahir/deedax/TLDR/TLDR/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "  def __init__(self, config: ModelTrainerConfig):\n",
    "    self.config = config\n",
    "\n",
    "  def train(self):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "    seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    dataset = load_from_disk(self.config.data_path)\n",
    "\n",
    "    trainer_args = TrainingArguments(\n",
    "      output_dir = self.config.root_dir,\n",
    "      num_train_epochs = self.config.num_train_epochs,\n",
    "      per_device_train_batch_size = self.config.per_device_train_batch_size,\n",
    "      warmup_steps = self.config.warmup_steps,\n",
    "      weight_decay = self.config.weight_decay,\n",
    "      logging_steps = self.config.logging_steps,\n",
    "      evaluation_strategy = self.config.evaluation_strategy,\n",
    "      eval_steps = self.config.eval_steps,\n",
    "      save_steps = self.config.save_steps,\n",
    "      gradient_accumulation_steps = self.config.gradient_accumulation_steps,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "      model = model,\n",
    "      args = trainer_args,\n",
    "      tokenizer = tokenizer,\n",
    "      train_dataset = dataset['test'],\n",
    "      eval_dataset = dataset['validation'],\n",
    "      data_collator = seq2seq_data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    model.save_pretrained(os.path.join(self.config.root_dir, 'model'))\n",
    "\n",
    "    tokenizer.save_pretrained(os.path.join(self.config.root_dir, 'tokenizer')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-19 22:23:12,903: INFO: common] yaml file: config/config.yaml read successfully.]\n",
      "[2023-10-19 22:23:12,908: INFO: common] yaml file: params.yaml read successfully.]\n",
      "[2023-10-19 22:23:12,911: INFO: common] Created directory: artifacts]\n",
      "[2023-10-19 22:23:12,913: INFO: common] Created directory: artifacts/model_trainer]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dahir/deedax/TLDR/TLDR/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 08:40, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "  config = ConfigurationManager()\n",
    "  model_trainer_config = config.get_model_trainer_config()\n",
    "  model_trainer = ModelTrainer(model_trainer_config)\n",
    "  model_trainer.train()\n",
    "except Exception as e:\n",
    "  raise e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "  root_dir: Path\n",
    "  data_path: Path\n",
    "  model_path: Path\n",
    "  tokenizer_path: Path\n",
    "  metric_file_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TLDR.constants import *\n",
    "from TLDR.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "  def __init__(\n",
    "    self,\n",
    "    config_filepath = CONFIG_FILE_PATH,\n",
    "    params_filepath = PARAMS_FILE_PATH,\n",
    "  ):\n",
    "    self.config = read_yaml(config_filepath)\n",
    "    self.params = read_yaml(params_filepath)\n",
    "\n",
    "    create_directories([self.config.artifacts_root])\n",
    "\n",
    "  def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "    config = self.config.model_evaluation\n",
    "\n",
    "    create_directories([config.root_dir])\n",
    "    \n",
    "    model_evaluation_config = ModelEvaluationConfig(\n",
    "    root_dir = config.root_dir,\n",
    "    data_path = config.data_path,\n",
    "    model_path = config.model_path,\n",
    "    tokenizer_path = config.tokenizer_path,\n",
    "    metric_file_name = config.metric_file_name,\n",
    "    )\n",
    "\n",
    "    return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "  def __init__(self, config: ModelEvaluationConfig):\n",
    "    self.config = config\n",
    "\n",
    "  def generate_batch_sized_chunks(self, list_of_elements, batch_size):\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "      yield list_of_elements[i:i + batch_size]\n",
    "\n",
    "  def calculate_metric_on_test_ds(self, dataset, metric, model, tokenizer,\n",
    "                                  batch_size = 16, device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                                  column_text = 'article', column_summary = 'highlights'):\n",
    "    article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "      zip(article_batches, target_batches), total = len(article_batches)):\n",
    "      # Tokenize the batch of articles\n",
    "      inputs = tokenizer(article_batch, padding = 'max_length', truncation = True, max_length = 1024, return_tensors = 'pt')\n",
    "      summaries = model.generate(inputs['input_ids'].to(device), \n",
    "                                 attention_mask = inputs['attention_mask'].to(device),\n",
    "                                 length_penalty = 0.8, num_beams = 8, max_length = 128)\n",
    "      decoded_summaries = [\n",
    "        tokenizer.decode(s, skip_special_tokens = True, clean_up_tokenization_spaces = True) for s in summaries\n",
    "      ]\n",
    "      decoded_summaries = [d.replace('', ' ') for d in decoded_summaries]\n",
    "      metric.add_batch(predictions = decoded_summaries, references = target_batch)\n",
    "      score = metric.compute()\n",
    "      return score\n",
    "    \n",
    "  def evaluate(self):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)\n",
    "\n",
    "    dataset = load_from_disk(self.config.data_path)\n",
    "\n",
    "    rouge_names = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "    rouge = load_metric('rouge')\n",
    "    \n",
    "    score = self.calculate_metric_on_test_ds(dataset['test'][0:10], rouge, model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary = 'summary')\n",
    "    \n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "    \n",
    "    df = pd.DataFrame(rouge_dict, index=['flan-t5-small'])\n",
    "    df.to_csv(self.config.metric_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-20 18:34:07,078: INFO: common] yaml file: config/config.yaml read successfully.]\n",
      "[2023-10-20 18:34:07,085: INFO: common] yaml file: params.yaml read successfully.]\n",
      "[2023-10-20 18:34:07,087: INFO: common] Created directory: artifacts]\n",
      "[2023-10-20 18:34:07,088: INFO: common] Created directory: artifacts/model_evaluation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-20 18:34:20,269: INFO: rouge_scorer] Using default tokenizer.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:08<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  config = ConfigurationManager()\n",
    "  model_evaluation_config = config.get_model_evaluation_config()\n",
    "  model_evaluation = ModelEvaluation(model_evaluation_config)\n",
    "  model_evaluation.evaluate()\n",
    "except Exception as e:\n",
    "  raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TLDR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
